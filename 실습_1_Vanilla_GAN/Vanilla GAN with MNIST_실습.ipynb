{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> \n",
    "    <h1>LAB1 Vanilla GAN with MNIST dataset</h1>\n",
    "</center>\n",
    "\n",
    "* without any customization\" 이라는 의미로 사용, 또는 \"deault\", \"ordinary\", \"basic\" 라고 이해"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Lab 개 요 </h2>\n",
    "\n",
    "* GAN 개념과 구조 이해  \n",
    "\n",
    "* MNIST dataset을 이용한 vanilla GAN 구현  \n",
    "  . MNIST : 0 ~ 9 숫자를 여러 스타일의 손글씨로 쓴 것  \n",
    "\n",
    "* GAN 으로 FAKE 손글씨 숫자 생성(거의 똑 같은 수준 목표) \n",
    "\n",
    "<h3 style='color:green'> GAN 구조 </h3>\n",
    "<img src=\"../Figures/GAN.png\" width=700px>\n",
    "\n",
    "<h3 style='color:green'> 중점학습내용</h3>\n",
    "\n",
    "* Dataset 준비  \n",
    "* 필요 Libs Import  \n",
    "* Hyperparameters 설정  \n",
    "* Generator and Discriminator  \n",
    "* 시각화(Visualization)\n",
    "* Training  \n",
    "    \n",
    "GAN 이해 참고자료(https://velog.io/@tobigs-gm1/basicofgan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Dataset 준비</h2>\n",
    "\n",
    "mnist dataset을 다운로드 받기위해 tensorflow.keras.datasets 에서 mnist 을 import\n",
    "\n",
    "다음 코드 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\miniconda3\\envs\\jeff_heaton_t81_park\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Owner\\miniconda3\\envs\\jeff_heaton_t81_park\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Owner\\miniconda3\\envs\\jeff_heaton_t81_park\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Owner\\miniconda3\\envs\\jeff_heaton_t81_park\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Owner\\miniconda3\\envs\\jeff_heaton_t81_park\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Owner\\miniconda3\\envs\\jeff_heaton_t81_park\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from  tensorflow.keras.datasets  import  mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, _), (_, _) = mnist.load_data()   # 0 - 255  x_train에 load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train/127.5 - 1                # 평균값(0 - 2) - 1 = -1 ~ 1 normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.0, 1.0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.min(), x_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = x_train.reshape(-1, 784)     # 28 x 28  사이즈 이미지를  1-Dim Vec 로 flatten => 784\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>필요 모듈 Import</h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU, Dropout, Input\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Hyperparameters 설정</h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOISE_DIM = 10                      # gan input noise dimension  \n",
    "adam = Adam(lr=0.0002, beta_1=0.5)  # adam optimizer, learning_rate, beta_1 값 설정 (fast convergence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Generator</h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Sequential([\n",
    "    Dense(256, input_dim=NOISE_DIM), \n",
    "    LeakyReLU(0.2), \n",
    "    Dense(512), \n",
    "    LeakyReLU(0.2), \n",
    "    Dense(1024), \n",
    "    LeakyReLU(0.2), \n",
    "    Dense(28*28, activation='tanh'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 256)               2816      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 784)               803600    \n",
      "=================================================================\n",
      "Total params: 1,463,312\n",
      "Trainable params: 1,463,312\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Discriminator</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = Sequential([\n",
    "    Dense(1024, input_shape=(784,), kernel_initializer=RandomNormal(stddev=0.02)),\n",
    "    LeakyReLU(0.2), \n",
    "    Dropout(0.3), \n",
    "    Dense(512),\n",
    "    LeakyReLU(0.2), \n",
    "    Dropout(0.3), \n",
    "    Dense(256),\n",
    "    LeakyReLU(0.2), \n",
    "    Dropout(0.3), \n",
    "    Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 1024)              803840    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 1,460,225\n",
      "Trainable params: 1,460,225\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.compile(loss='binary_crossentropy', optimizer=adam) # discriminator 훈련을 위한 loss function / optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.trainable = False       # generator 훈련시 discriminator는 훈련하지 않음(고정, fixed)\n",
    "gan_input = Input(shape=(NOISE_DIM,))\n",
    "x = generator(inputs=gan_input)       # generator output => x : discriminator input\n",
    "output = discriminator(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan = Model(gan_input, output)        # gan model 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 10)]              0         \n",
      "_________________________________________________________________\n",
      "sequential (Sequential)      (None, 784)               1463312   \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (None, 1)                 1460225   \n",
      "=================================================================\n",
      "Total params: 2,923,537\n",
      "Trainable params: 1,463,312\n",
      "Non-trainable params: 1,460,225\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.compile(loss='binary_crossentropy', optimizer=adam)  # generator 훈련을 위한 loss function / optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(data, batch_size):   # 이미지 batch\n",
    "    batches = []\n",
    "    for i in range(int(data.shape[0] // batch_size)):\n",
    "        batch = data[i * batch_size: (i + 1) * batch_size]\n",
    "        batches.append(batch)\n",
    "    return np.asarray(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>시각화(Visualization)</h2>\n",
    "\n",
    "생성 이미지와 loss function 시각화를 위한 function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_training(epoch, d_losses, g_losses):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(d_losses, label='Discriminator Loss')\n",
    "    plt.plot(g_losses, label='Generatror Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    print('epoch: {}, Discriminator Loss: {}, Generator Loss: {}'.format(epoch, np.asarray(d_losses).mean(), np.asarray(g_losses).mean()))\n",
    "    \n",
    "    #Visualize after creating sample data\n",
    "    noise = np.random.normal(0, 1, size=(24, NOISE_DIM))\n",
    "    generated_images = generator.predict(noise)\n",
    "    generated_images = generated_images.reshape(-1, 28, 28)\n",
    "    \n",
    "    plt.figure(figsize=(8, 4))\n",
    "    for i in range(generated_images.shape[0]):\n",
    "        plt.subplot(4, 6, i+1)\n",
    "        plt.imshow(generated_images[i], interpolation='nearest', cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Training </h2>\n",
    "\n",
    "<img src=\"../Figures/d_g_loss.PNG\" width=600 height=600 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS= 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a list for measuring loss of discriminator and gan model.\n",
    "d_losses = []\n",
    "g_losses = []\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    # train for each batch\n",
    "    for real_images in get_batches(x_train, BATCH_SIZE):\n",
    "        # Generate random noise\n",
    "        input_noise = np.random.uniform(-1, 1, size=[BATCH_SIZE, NOISE_DIM])\n",
    "        \n",
    "        # Generate fake image data\n",
    "        generated_images = generator.predict(input_noise)\n",
    "        \n",
    "        # Define X data to learn on Gan\n",
    "        x_dis = np.concatenate([real_images, generated_images])\n",
    "        \n",
    "        # Define Y data to be trained\n",
    "        y_dis = np.zeros(2 * BATCH_SIZE)\n",
    "        y_dis[:BATCH_SIZE] = 0.9\n",
    "        \n",
    "        # Discriminator training\n",
    "        discriminator.trainable = True\n",
    "        d_loss = discriminator.train_on_batch(x_dis, y_dis)\n",
    "        \n",
    "        # Gan training\n",
    "        noise = np.random.uniform(-1, 1, size=[BATCH_SIZE, NOISE_DIM])\n",
    "        y_gan = np.ones(BATCH_SIZE)\n",
    "        \n",
    "        # Train Generator\n",
    "        discriminator.trainable = False\n",
    "        g_loss = gan.train_on_batch(noise, y_gan)\n",
    "        \n",
    "    d_losses.append(d_loss)\n",
    "    g_losses.append(g_loss)\n",
    "\n",
    "    if epoch == 1 or epoch % 5 == 0:\n",
    "        visualize_training(epoch, d_losses, g_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"result/result.png\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Lab1 Vanilla GAN with MNIST_실습종료 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "jeff_heaton_t81_park"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
